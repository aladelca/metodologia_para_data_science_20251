{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Series de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sp500 = pd.read_csv(\"../data/processed/sp500_index.csv\")\n",
    "data_sp500.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (30, 5))\n",
    "sns.lineplot(x = \"Date\", y = \"S&P500\", data = data_sp500)\n",
    "plt.title(\"S&P500 Index\")\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sp500.set_index(\"Date\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "result = seasonal_decompose(data_sp500, model='multiplicative', period=1)\n",
    "result.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinar si una serie es estacionaria\n",
    "\n",
    "from typing import Tuple, Optional\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "def test_estacionariedad(\n",
    "    serie: pd.Series,\n",
    "    ventana: int = 12,\n",
    "    titulo: str = \"Análisis de Estacionariedad\",\n",
    "    figsize: Tuple[int, int] = (12, 8)\n",
    ") -> Tuple[bool, float, dict]:\n",
    "    \"\"\"Realiza el test de Dickey-Fuller aumentado para determinar estacionariedad.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    serie : pd.Series\n",
    "        Serie temporal a analizar.\n",
    "    ventana : int, optional\n",
    "        Tamaño de la ventana para el cálculo de la media móvil, por defecto 12.\n",
    "    titulo : str, optional\n",
    "        Título para la gráfica, por defecto \"Análisis de Estacionariedad\".\n",
    "    figsize : Tuple[int, int], optional\n",
    "        Tamaño de la figura, por defecto (12, 8).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[bool, float, dict]\n",
    "        - bool: True si la serie es estacionaria (p-valor < 0.05)\n",
    "        - float: p-valor del test\n",
    "        - dict: Resultados completos del test ADF\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> import numpy as np\n",
    "    >>> # Crear una serie temporal no estacionaria\n",
    "    >>> serie = pd.Series(np.random.normal(0, 1, 100).cumsum())\n",
    "    >>> es_estacionaria, p_valor, resultados = test_estacionariedad(serie)\n",
    "    >>> print(f\"¿Es estacionaria? {es_estacionaria}\")\n",
    "    >>> print(f\"p-valor: {p_valor:.4f}\")\n",
    "    \"\"\"\n",
    "    # Calcular estadísticas móviles\n",
    "    media_movil = serie.rolling(window=ventana).mean()\n",
    "    std_movil = serie.rolling(window=ventana).std()\n",
    "\n",
    "    # Realizar el test ADF\n",
    "    resultado_adf = adfuller(serie.dropna())\n",
    "    \n",
    "    # Extraer resultados\n",
    "    estadistico_adf = resultado_adf[0]\n",
    "    p_valor = resultado_adf[1]\n",
    "    valores_criticos = resultado_adf[4]\n",
    "    \n",
    "    # Crear diccionario con resultados\n",
    "    resultados = {\n",
    "        'estadistico_adf': estadistico_adf,\n",
    "        'p_valor': p_valor,\n",
    "        'valores_criticos': valores_criticos,\n",
    "        'n_observaciones': resultado_adf[3],\n",
    "        'orden_maximo': resultado_adf[2]\n",
    "    }\n",
    "\n",
    "    # Visualizar resultados\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Gráfica de la serie original y media móvil\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(serie, label='Serie Original')\n",
    "    plt.plot(media_movil, label=f'Media Móvil ({ventana} períodos)')\n",
    "    plt.title('Serie Temporal y Media Móvil')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Gráfica de la desviación estándar móvil\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(std_movil, label=f'Desviación Estándar Móvil ({ventana} períodos)')\n",
    "    plt.title('Desviación Estándar Móvil')\n",
    "    plt.legend()\n",
    "    \n",
    "    # ACF\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plot_acf(serie, ax=plt.gca(), lags=40)\n",
    "    plt.title('Función de Autocorrelación (ACF)')\n",
    "    \n",
    "    # PACF\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plot_pacf(serie, ax=plt.gca(), lags=40)\n",
    "    plt.title('Función de Autocorrelación Parcial (PACF)')\n",
    "    \n",
    "    plt.suptitle(titulo)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Imprimir resultados del test\n",
    "    print('Resultados del Test de Dickey-Fuller Aumentado:')\n",
    "    print(f'Estadístico ADF: {estadistico_adf:.4f}')\n",
    "    print(f'p-valor: {p_valor:.4f}')\n",
    "    print('Valores Críticos:')\n",
    "    for key, value in valores_criticos.items():\n",
    "        print(f'\\t{key}: {value:.4f}')\n",
    "    \n",
    "    # Determinar si la serie es estacionaria\n",
    "    es_estacionaria = p_valor < 0.05\n",
    "    \n",
    "    print(f'\\nConclusión: La serie es {\"estacionaria\" if es_estacionaria else \"no estacionaria\"}')\n",
    "    print(f'(p-valor {\"<\" if es_estacionaria else \">\"} 0.05)')\n",
    "\n",
    "    return es_estacionaria, p_valor, resultados\n",
    "\n",
    "\n",
    "test_estacionariedad(data_sp500[\"S&P500\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_sp500 = data_sp500.diff(1).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = seasonal_decompose(diff_sp500, model='additive', period=1)\n",
    "result.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_estacionariedad(diff_sp500[\"S&P500\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fecha máxima: \", np.max(diff_sp500.index))\n",
    "print(\"Fecha mínima: \", np.min(diff_sp500.index))\n",
    "\n",
    "\n",
    "\n",
    "test = diff_sp500.loc[diff_sp500.index >= \"2024-01-01\"]\n",
    "train = diff_sp500.loc[(diff_sp500.index < \"2024-01-01\") & (diff_sp500.index >= \"2021-01-01\")]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos arima\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "mod = sm.tsa.arima.ARIMA(train, order=(0, 0, 0))\n",
    "res = mod.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = res.predict(start=len(train.index)+1, end=len(train.index) + len(test.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valores_reales = data_sp500.loc[train.index]\n",
    "test_valores_reales = data_sp500.loc[test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_base = train_valores_reales.tail(1).values[0][0]\n",
    "preds_finales = []\n",
    "for i in preds:\n",
    "    valor_predicho = valor_base + i\n",
    "    valor_base = valor_predicho\n",
    "    preds_finales.append(valor_predicho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(test_valores_reales, preds_finales)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x = \"Date\", y = \"S&P500\", data = train_valores_reales, label = \"Train\")\n",
    "sns.lineplot(x = \"Date\", y = \"S&P500\", data = test_valores_reales, label = \"Test\")\n",
    "sns.lineplot(x = test_valores_reales.index, y = preds_finales, label = \"Predictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x = \"Date\", y = \"S&P500\", data = test_valores_reales, label = \"Test\")\n",
    "sns.lineplot(x = test_valores_reales.index, y = preds_finales, label = \"Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, Any, Optional, List\n",
    "import itertools\n",
    "import warnings\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "def encontrar_mejor_arima(\n",
    "    train,\n",
    "    test,\n",
    "    max_p: int = 5,\n",
    "    max_d: int = 2,\n",
    "    max_q: int = 5,\n",
    "    max_P: int = 2,\n",
    "    max_D: int = 1,\n",
    "    max_Q: int = 2,\n",
    "    m: int = 12,\n",
    "    seasonal: bool = False,\n",
    "    information_criterion: str = 'aic'\n",
    ") -> Tuple[Tuple[int, int, int], Tuple[int, int, int], float]:\n",
    "    \"\"\"Encuentra los mejores parámetros para el modelo ARIMA/SARIMA.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    serie : pd.Series\n",
    "        Serie temporal a modelar.\n",
    "    max_p : int, optional\n",
    "        Máximo orden del componente AR, por defecto 5.\n",
    "    max_d : int, optional\n",
    "        Máximo orden de diferenciación, por defecto 2.\n",
    "    max_q : int, optional\n",
    "        Máximo orden del componente MA, por defecto 5.\n",
    "    max_P : int, optional\n",
    "        Máximo orden del componente AR estacional, por defecto 2.\n",
    "    max_D : int, optional\n",
    "        Máximo orden de diferenciación estacional, por defecto 1.\n",
    "    max_Q : int, optional\n",
    "        Máximo orden del componente MA estacional, por defecto 2.\n",
    "    m : int, optional\n",
    "        Período estacional, por defecto 12.\n",
    "    seasonal : bool, optional\n",
    "        Si se debe considerar estacionalidad, por defecto True.\n",
    "    information_criterion : str, optional\n",
    "        Criterio de información a usar ('aic' o 'bic'), por defecto 'aic'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Tuple[int, int, int], Tuple[int, int, int], float]\n",
    "        - Mejores parámetros (p,d,q)\n",
    "        - Mejores parámetros estacionales (P,D,Q)\n",
    "        - Valor del criterio de información\n",
    "    \"\"\"\n",
    "    mejor_criterio = float('inf')\n",
    "    mejores_parametros = None\n",
    "    mejores_parametros_estacionales = None\n",
    "\n",
    "    # Determinar el orden de diferenciación\n",
    "    d = 0\n",
    "\n",
    "    # Generar combinaciones de parámetros\n",
    "    p_values = range(max_p + 1)\n",
    "    q_values = range(max_q + 1)\n",
    "    P_values = range(max_P + 1) if seasonal else [0]\n",
    "    Q_values = range(max_Q + 1) if seasonal else [0]\n",
    "    D_values = range(max_D + 1) if seasonal else [0]\n",
    "\n",
    "    # Probar todas las combinaciones\n",
    "    for p, q, P, Q, D in itertools.product(p_values, q_values, P_values, Q_values, Q_values):\n",
    "        try:\n",
    "            modelo = SARIMAX(\n",
    "                train,\n",
    "                order=(p, d, q),\n",
    "                seasonal_order=(P, D, Q, m) if seasonal else (0, 0, 0, 0)\n",
    "            )\n",
    "            resultados = modelo.fit(disp=False)\n",
    "            \n",
    "            # Obtener criterio de información\n",
    "            criterio = getattr(resultados, information_criterion)\n",
    "            \n",
    "            if criterio < mejor_criterio:\n",
    "                mejor_criterio = criterio\n",
    "                mejores_parametros = (p, d, q)\n",
    "                mejores_parametros_estacionales = (P, D, Q)\n",
    "                \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return mejores_parametros, mejores_parametros_estacionales, mejor_criterio\n",
    "\n",
    "encontrar_mejor_arima(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exponential Smoothing\n",
    "\n",
    "from statsmodels.tsa.api import ExponentialSmoothing\n",
    "fit1 = ExponentialSmoothing(\n",
    "    train,\n",
    "    seasonal_periods=10,\n",
    "    trend=\"add\",\n",
    "    seasonal=\"add\"\n",
    ").fit()\n",
    "\n",
    "preds = fit1.predict(start=len(train.index)+1, end=len(train.index) + len(test.index))\n",
    "\n",
    "valor_base = train_valores_reales.tail(1).values[0][0]\n",
    "preds_finales = []\n",
    "for i in preds:\n",
    "    valor_predicho = valor_base + i\n",
    "    valor_base = valor_predicho\n",
    "    preds_finales.append(valor_predicho)\n",
    "\n",
    "mean_absolute_error(test_valores_reales, preds_finales)\n",
    "\n",
    "\n",
    "sns.lineplot(x = \"Date\", y = \"S&P500\", data = test_valores_reales, label = \"Test\")\n",
    "sns.lineplot(x = test_valores_reales.index, y = preds_finales, label = \"Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valores_reales.reset_index(inplace=True)\n",
    "train_valores_reales.columns = [\"ds\", \"y\"]\n",
    "test_valores_reales.reset_index(inplace=True)\n",
    "test_valores_reales.columns = [\"ds\", \"y\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valores_reales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "m = Prophet()\n",
    "m.fit(train_valores_reales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = m.predict(test_valores_reales)\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_absolute_error(test_valores_reales[\"y\"], df_pred[\"yhat\"]))\n",
    "\n",
    "sns.lineplot(x = \"ds\", y = \"y\", data = test_valores_reales, label = \"Test\")\n",
    "sns.lineplot(x = test_valores_reales[\"ds\"], y = df_pred[\"yhat\"], label = \"Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Probando con LSTM\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from typing import Tuple, Dict, List, Any\n",
    "\n",
    "\n",
    "def crear_dataset(\n",
    "    serie: np.ndarray, \n",
    "    ventana: int = 1\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Crea conjuntos de datos X e y para entrenamiento del LSTM.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    serie : np.ndarray\n",
    "        Serie temporal univariada.\n",
    "    ventana : int, optional\n",
    "        Tamaño de la ventana (lookback), por defecto 1.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, np.ndarray]\n",
    "        X: Datos de entrada de forma (n_muestras, ventana, 1)\n",
    "        y: Datos objetivo\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(serie) - ventana):\n",
    "        X.append(serie[i:(i + ventana)])\n",
    "        y.append(serie[i + ventana])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def preparar_datos(\n",
    "    serie: pd.Series,\n",
    "    ventana: int = 6,\n",
    "    fecha_inicio: str = \"2020-01-01\",\n",
    "    escalar: Any = None\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Any]:\n",
    "    \"\"\"Prepara los datos para el modelo LSTM.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    serie : pd.Series\n",
    "        Serie temporal a modelar.\n",
    "    ventana : int, optional\n",
    "        Tamaño de la ventana, por defecto 6.\n",
    "    train_size : float, optional\n",
    "        Proporción de datos para entrenamiento, por defecto 0.8.\n",
    "    escalar : Any, optional\n",
    "        Escalador preentrenado. Si es None, se crea uno nuevo.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Any]\n",
    "        X_train, X_test, y_train, y_test, escalar\n",
    "    \"\"\"\n",
    "    # Convertir a array y reshape para el escalador\n",
    "    valores = serie.values.reshape(-1, 1)\n",
    "    \n",
    "    # Escalar los datos\n",
    "    if escalar is None:\n",
    "        escalar = MinMaxScaler(feature_range=(0, 1))\n",
    "        valores_escalados = pd.DataFrame(escalar.fit_transform(valores), index=serie.index)\n",
    "    else:\n",
    "        valores_escalados = pd.DataFrame(escalar.transform(valores), index=serie.index)\n",
    "    \n",
    "    # Dividir en train y test\n",
    "\n",
    "    train = valores_escalados.iloc[valores_escalados.index < fecha_inicio]\n",
    "    test = valores_escalados[valores_escalados.index >= fecha_inicio]\n",
    " \n",
    "    # Crear datasets para LSTM\n",
    "    X_train, y_train = crear_dataset(train.values, ventana)\n",
    "    X_test, y_test = crear_dataset(test.values, ventana)\n",
    "    \n",
    "    # Reshape para LSTM [muestras, pasos_tiempo, características]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, escalar\n",
    "\n",
    "\n",
    "def crear_modelo_lstm(\n",
    "    ventana: int = 6,\n",
    "    unidades: int = 50,\n",
    "    capas: int = 1,\n",
    "    dropout: float = 0.2\n",
    ") -> Sequential:\n",
    "    \"\"\"Crea un modelo LSTM para series temporales.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ventana : int, optional\n",
    "        Tamaño de la ventana, por defecto 6.\n",
    "    unidades : int, optional\n",
    "        Número de unidades en cada capa LSTM, por defecto 50.\n",
    "    capas : int, optional\n",
    "        Número de capas LSTM, por defecto 1.\n",
    "    dropout : float, optional\n",
    "        Tasa de dropout, por defecto 0.2.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Sequential\n",
    "        Modelo Keras LSTM\n",
    "    \"\"\"\n",
    "    modelo = Sequential()\n",
    "    \n",
    "    # Primera capa LSTM\n",
    "    if capas == 1:\n",
    "        modelo.add(LSTM(units=unidades, input_shape=(ventana, 1)))\n",
    "    else:\n",
    "        modelo.add(LSTM(units=unidades, return_sequences=True, input_shape=(ventana, 1)))\n",
    "        modelo.add(Dropout(dropout))\n",
    "        \n",
    "        # Capas intermedias\n",
    "        for _ in range(capas - 2):\n",
    "            modelo.add(LSTM(units=unidades, return_sequences=True))\n",
    "            modelo.add(Dropout(dropout))\n",
    "        \n",
    "        # Última capa LSTM\n",
    "        modelo.add(LSTM(units=unidades))\n",
    "    \n",
    "    modelo.add(Dropout(dropout))\n",
    "    modelo.add(Dense(units=1))\n",
    "    \n",
    "    modelo.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    return modelo\n",
    "\n",
    "\n",
    "def entrenar_lstm(\n",
    "    modelo: Sequential,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    epocas: int = 100,\n",
    "    batch_size: int = 32,\n",
    "    paciencia: int = 10,\n",
    "    verbose: int = 1,\n",
    "    ruta_modelo: str = 'mejor_modelo.h5'\n",
    ") -> Tuple[Sequential, Dict[str, List[float]]]:\n",
    "    \"\"\"Entrena el modelo LSTM.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    modelo : Sequential\n",
    "        Modelo Keras LSTM.\n",
    "    X_train : np.ndarray\n",
    "        Datos de entrenamiento X.\n",
    "    y_train : np.ndarray\n",
    "        Datos de entrenamiento y.\n",
    "    X_test : np.ndarray\n",
    "        Datos de validación X.\n",
    "    y_test : np.ndarray\n",
    "        Datos de validación y.\n",
    "    epocas : int, optional\n",
    "        Número de épocas de entrenamiento, por defecto 100.\n",
    "    batch_size : int, optional\n",
    "        Tamaño del lote, por defecto 32.\n",
    "    paciencia : int, optional\n",
    "        Paciencia para early stopping, por defecto 10.\n",
    "    verbose : int, optional\n",
    "        Nivel de verbosidad, por defecto 1.\n",
    "    ruta_modelo : str, optional\n",
    "        Ruta para guardar el mejor modelo, por defecto 'mejor_modelo.h5'.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Sequential, Dict[str, List[float]]]\n",
    "        Modelo entrenado e historial de entrenamiento\n",
    "    \"\"\"\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=paciencia,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        filepath=ruta_modelo,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    )\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    historia = modelo.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epocas,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping, model_checkpoint],\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    return modelo, historia.history\n",
    "\n",
    "\n",
    "def evaluar_predicciones(\n",
    "    y_real: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    escalar: Any = None\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evalúa las predicciones del modelo.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_real : np.ndarray\n",
    "        Valores reales.\n",
    "    y_pred : np.ndarray\n",
    "        Valores predichos.\n",
    "    escalar : Any, optional\n",
    "        Escalador para invertir la transformación, por defecto None.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, float]\n",
    "        Diccionario con métricas de evaluación\n",
    "    \"\"\"\n",
    "    # Invertir la transformación si se proporciona un escalador\n",
    "    if escalar is not None:\n",
    "        y_real = escalar.inverse_transform(y_real.reshape(-1, 1)).flatten()\n",
    "        y_pred = escalar.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Calcular métricas\n",
    "    metricas = {\n",
    "        'mse': mean_squared_error(y_real, y_pred),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_real, y_pred)),\n",
    "        'mae': mean_absolute_error(y_real, y_pred),\n",
    "        'r2': r2_score(y_real, y_pred)\n",
    "    }\n",
    "    \n",
    "    return metricas\n",
    "\n",
    "\n",
    "def visualizar_resultados(\n",
    "    serie: pd.Series,\n",
    "    real: np.ndarray,\n",
    "    predicciones: np.ndarray,\n",
    "    titulo: str = \"Predicciones LSTM\",\n",
    "    figsize: Tuple[int, int] = (12, 6)\n",
    ") -> None:\n",
    "    \"\"\"Visualiza los resultados del modelo.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    serie : pd.Series\n",
    "        Serie temporal original.\n",
    "    real : np.ndarray\n",
    "        Valores reales.\n",
    "    predicciones : np.ndarray\n",
    "        Valores predichos.\n",
    "    titulo : str, optional\n",
    "        Título del gráfico, por defecto \"Predicciones LSTM\".\n",
    "    figsize : Tuple[int, int], optional\n",
    "        Tamaño de la figura, por defecto (12, 6).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Graficar serie original\n",
    "    plt.plot(serie, label='Serie Original', alpha=0.5)\n",
    "    \n",
    "    # Calcular el índice correcto para las predicciones\n",
    "    offset = len(serie) - len(real)\n",
    "    idx_predicciones = range(offset, offset + len(predicciones))\n",
    "    \n",
    "    # Graficar predicciones\n",
    "    plt.plot(idx_predicciones, predicciones, label='Predicciones', color='red')\n",
    "    \n",
    "    plt.title(titulo)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def pronosticar_futuro(\n",
    "    modelo: Sequential,\n",
    "    serie: np.ndarray,\n",
    "    ventana: int,\n",
    "    n_pasos: int,\n",
    "    escalar: Any\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Realiza pronósticos futuros con el modelo LSTM.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    modelo : Sequential\n",
    "        Modelo LSTM entrenado.\n",
    "    serie : np.ndarray\n",
    "        Serie temporal escalada.\n",
    "    ventana : int\n",
    "        Tamaño de la ventana.\n",
    "    n_pasos : int\n",
    "        Número de pasos a pronosticar.\n",
    "    escalar : Any\n",
    "        Escalador para invertir la transformación.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Array con pronósticos\n",
    "    \"\"\"\n",
    "    # Obtener los últimos 'ventana' valores para la predicción inicial\n",
    "    ultimos_valores = serie[-ventana:].reshape(1, ventana, 1)\n",
    "    \n",
    "    # Lista para almacenar pronósticos\n",
    "    pronosticos = []\n",
    "    \n",
    "    # Realizar pronósticos paso a paso\n",
    "    for _ in range(n_pasos):\n",
    "        # Predecir el siguiente valor\n",
    "        siguiente_valor = modelo.predict(ultimos_valores, verbose=0)\n",
    "        pronosticos.append(siguiente_valor[0, 0])\n",
    "        \n",
    "        # Actualizar los valores para la siguiente predicción\n",
    "        ultimos_valores = np.append(ultimos_valores[:, 1:, :], \n",
    "                                    siguiente_valor.reshape(1, 1, 1), \n",
    "                                    axis=1)\n",
    "    \n",
    "    # Convertir a array y deshacer escalado\n",
    "    pronosticos = np.array(pronosticos).reshape(-1, 1)\n",
    "    pronosticos = escalar.inverse_transform(pronosticos).flatten()\n",
    "    \n",
    "    return pronosticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "train_lstm = train_valores_reales.set_index(\"ds\")\n",
    "test_lstm = test_valores_reales.set_index(\"ds\")\n",
    "esc = MinMaxScaler()\n",
    "\n",
    "train_lstm[\"y\"] = esc.fit_transform(train_lstm[\"y\"].values.reshape(-1, 1))\n",
    "test_lstm[\"y\"] = esc.transform(test_lstm[\"y\"].values.reshape(-1, 1))\n",
    "x_train, y_train = crear_dataset(train_lstm[\"y\"].values, ventana=1)\n",
    "x_test, y_test = crear_dataset(test_lstm[\"y\"].values, ventana=1)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = Sequential()\n",
    "    \n",
    "    \n",
    "    \n",
    "modelo.add(LSTM(units=256, return_sequences=True, input_shape=(1, 1)))\n",
    "modelo.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "modelo.add(LSTM(units=512, return_sequences=True))\n",
    "modelo.add(Dropout(0.2))\n",
    "\n",
    "# Última capa LSTM\n",
    "modelo.add(LSTM(units=512))\n",
    "\n",
    "modelo.add(Dropout(0.2))\n",
    "modelo.add(Dense(units=1))\n",
    "\n",
    "modelo.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "modelo.fit(x_train, y_train, epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = esc.inverse_transform(modelo.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(preds, test_valores_reales[\"y\"].iloc[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sns.lineplot(x = \"ds\", y = \"y\", data = test_valores_reales.iloc[test_valores_reales[\"y\"].iloc[:-1].index], label = \"Test\")\n",
    "\n",
    "\n",
    "\n",
    "sns.lineplot(x = test_valores_reales[\"ds\"].iloc[:-1], y = preds[:,0], label = \"Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenando el modelo para acciones\n",
    "\n",
    "\n",
    "data_stocks = pd.read_csv(\"../data/processed/sp500_stocks.csv\")\n",
    "data_stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acciones_disponibles = data_stocks[\"Symbol\"].unique()\n",
    "\n",
    "amazon = data_stocks[data_stocks[\"Symbol\"] == \"NVDA\"]\n",
    "amazon = amazon.set_index(\"Date\")\n",
    "X_train, X_test, y_train, y_test, escalar = preparar_datos(amazon[\"Close\"],1,\"2024-01-01\")\n",
    "\n",
    "modelo = Sequential()\n",
    "    \n",
    "\n",
    "modelo.add(LSTM(units=256, return_sequences=True, input_shape=(1, 1)))\n",
    "modelo.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "modelo.add(LSTM(units=512, return_sequences=True))\n",
    "modelo.add(Dropout(0.2))\n",
    "\n",
    "# Última capa LSTM\n",
    "modelo.add(LSTM(units=512))\n",
    "\n",
    "modelo.add(Dropout(0.2))\n",
    "modelo.add(Dense(units=1))\n",
    "\n",
    "modelo.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "modelo.fit(X_train, y_train, epochs=50, batch_size=128)\n",
    "\n",
    "preds = escalar.inverse_transform(modelo.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data = amazon.iloc[(amazon.index >= \"2024-01-01\") & (amazon.index < np.max(amazon.index))], x = amazon.iloc[(amazon.index >= \"2024-01-01\") & (amazon.index < np.max(amazon.index))].index, y = amazon.iloc[(amazon.index >= \"2024-01-01\") & (amazon.index < np.max(amazon.index))][\"Close\"], label = \"Real\")\n",
    "sns.lineplot(x = amazon.iloc[(amazon.index >= \"2024-01-01\") & (amazon.index < np.max(amazon.index))].index, y = preds[:,0], label = \"Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "last_value_available = np.array([[[1.87781358]]])\n",
    "predictions = []\n",
    "for i in range(n):\n",
    "    last_value_available = modelo.predict(np.array(last_value_available))\n",
    "    last_value_available = last_value_available.reshape(1,1,1)\n",
    "    predictions.append(escalar.inverse_transform(last_value_available.reshape(1,1))[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x = range(0, len(predictions)), y = predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
